# -*- coding: utf-8 -*-
"""CONVOLUTIONAL NEURAL NETWORK APPLICATION

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EB4604qFBx1QGtaBEomzMGp6bxQml7lt
"""

import tensorflow as tf

tf.__version__

# Import the MNIST dataset
#from tensorflow.examples.tutorials.mnist import input_data
#mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

import tensorflow_datasets
mnist = tensorflow_datasets.load('mnist')

#Creating an interactive section , [Interactive session] create your coding and run on the fly.
import tensorflow.compat.v1 as tfc
sess = tfc.InteractiveSession()

# Creating placeholders (inputs ("Xs") and outputs ("Ys"))
tf.compat.v1.disable_eager_execution()
X = tf.compat.v1.placeholder(dtype= tf.float32 , shape = [None ,784])
Y = tf.compat.v1.placeholder(dtype= tf.float32 , shape = [None,10])

# Assigning bias and weights
W = tf.Variable(tf.zeros(dtype = tf.float32 , shape = [784, 10]))
B = tf.Variable(tf.zeros(dtype= tf.float32 , shape=[10] ))

#TensorFlow need to initialize the variables that you assign.

sess.run(tf.compat.v1.global_variables_initializer())

tf.matmul(X,W) + B   # mathematical operation to add weights and biases to the inputs

Y = tf.nn.softmax(tf.matmul(X,W) + B)  #Softmax activation function 
Y
#Softmax function is generalized type of logistic function. That is, Softmax can output a multiclass categorical probability distribution.

"""**Cost function** 
It is a function that is used to minimize the difference between the right answers (labels) and estimated outputs
"""

tf.compat.v1.disable_eager_execution()

cross_entropy = tf.reduce_mean(-tf.reduce_sum(Y*tf.compat.v1.log(Y)))

train_step = tf.compat.v1.train.GradientDescentOptimizer(0.5).minimize(cross_entropy )
#configure the optimizer for your Neural Network. There are several optimizers available, 
#in our case we will use Gradient Descent because it is a well established optimizer

for i in range(1000):
  batch = mnist.train.next_batch(50)
  train_step.run(feed_dict= {X : batch[0], Y : batch[1]})

correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
acc = accuracy.eval(feed_dict={X: mnist.test.images, Y: mnist.test.labels}) * 100
print("The final accuracy for the simple ANN model is: {} % ".format(acc) )

sess.close()